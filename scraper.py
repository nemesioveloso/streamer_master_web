import requests
import logging
from bs4 import BeautifulSoup
from models import MidiaBase
from urllib.parse import urljoin

BASE_URL = "https://publicdomainmovie.net"

# Configura√ß√£o do logger
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


def scrape_section(path: str):
    url = urljoin(BASE_URL, f"/{path}")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}

    filmes = []
    pagina = 1
    visitadas = set()

    while url and url not in visitadas:
        visitadas.add(url)
        logger.info(f"üîé Acessando {url}")
        resp = requests.get(url, headers=headers, timeout=15)
        if resp.status_code != 200:
            logger.error(f"‚ùå Erro ao acessar {url}: {resp.status_code}")
            break

        soup = BeautifulSoup(resp.text, "html.parser")

        # Cada ‚Äúbloco‚Äù de filme fica dentro de .views-field-nothing-1
        elementos = soup.select(".views-field-nothing-1")
        logger.info(f"üìÑ P√°gina {pagina} ‚Üí {len(elementos)} filmes encontrados")

        for bloco in elementos:
            link = bloco.select_one(".details h1 a")
            if not link:
                continue

            titulo = link.get_text(strip=True)
            href = urljoin(BASE_URL, link.get("href", ""))

            img_tag = bloco.select_one("img")
            imagem = (
                urljoin(BASE_URL, img_tag["src"])
                if img_tag and img_tag.get("src")
                else None
            )

            if not titulo:
                logger.warning(f"‚ö†Ô∏è Link sem t√≠tulo ignorado: {href}")
                continue

            filmes.append(
                MidiaBase(titulo=titulo, url=href, tipo="FILME", imagem=imagem)
            )
            logger.info(f"üé¨ Extra√≠do: {titulo} ({imagem}) -> {href}")

        # Pr√≥xima p√°gina (‚Äúnext ‚Ä∫‚Äù)
        next_el = soup.select_one("li.pager-next a")
        if next_el and next_el.get("href"):
            url = urljoin(BASE_URL, next_el["href"])
            pagina += 1
        else:
            url = None  # acabou a pagina√ß√£o

    logger.info(f"‚úÖ Total extra√≠do na se√ß√£o /{path}: {len(filmes)}")
    return filmes


def scrape_publicdomainmovies():
    """
    Faz scraping de m√∫ltiplas se√ß√µes e retorna tudo como FILME.
    """
    filmes = scrape_section("feature_movies")
    cartoons = scrape_section("cartoons")

    total = len(filmes) + len(cartoons)
    logger.info(f"üéâ Total geral extra√≠do: {total} filmes")
    return filmes + cartoons
